{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_csv('./data/users.csv')\n",
    "books = pd.read_csv('./data/books.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def process_context_data(users, books, ratings1, ratings2):\n",
    "\n",
    "users['location_city'] = users['location'].apply(lambda x: x.split(',')[0])\n",
    "users['location_state'] = users['location'].apply(lambda x: x.split(',')[1])\n",
    "users['location_country'] = users['location'].apply(lambda x: x.split(',')[2])\n",
    "users = users.drop(['location'], axis=1)\n",
    "\n",
    "######################### location 전처리\n",
    "users['location_city'] = users['location_city'].str.strip()\n",
    "users['location_state'] = users['location_state'].str.strip()\n",
    "users['location_country'] = users['location_country'].str.strip()\n",
    "users['location_city'] = users['location_city'].str.replace(r'[^a-zA-Z]', '', regex=True)\n",
    "users['location_state'] = users['location_state'].str.replace(r'[^a-zA-Z]', '', regex=True)\n",
    "users['location_country'] = users['location_country'].str.replace(r'[^a-zA-Z]', '', regex=True)\n",
    "'''\n",
    "location_country\n",
    "'''\n",
    "# null & na & universe & etc\n",
    "null_repl = [\n",
    "    'universe', 'na', '', 'lava', 'petrolwarnation', 'space', 'lachineternelle',\n",
    "    'faraway', 'everywhereandanywhere', 'hereandthere', 'tdzimi', 'naontheroad',\n",
    "    'unknown'\n",
    "]\n",
    "for keyword in null_repl:\n",
    "    users.loc[users['location_country'] == keyword, 'location_country'] = 'null'\n",
    "users.loc[users['location_country'] == 'c', 'location_country'] = 'null'\n",
    "# australia\n",
    "australia_repl = [\n",
    "    'newsouthwales', 'queensland', 'tasmania', 'victoria', 'nsw', 'southaustralia'\n",
    "]\n",
    "for keyword in australia_repl:\n",
    "    users.loc[users['location_country'].str.contains(keyword), 'location_country'] = 'australia'\n",
    "# italy\n",
    "users.loc[users['location_country'].str.contains('ital'), 'location_country'] = 'italy'\n",
    "users.loc[users['location_country'].str.contains('ferrara'), 'location_country'] = 'italy'\n",
    "users.loc[users['location_country'].str.contains('veneziagiulia'), 'location_country'] = 'italy'\n",
    "users.loc[users['location_country'].str.contains('ineurope'), 'location_country'] = 'italy'\n",
    "# germany\n",
    "users.loc[users['location_country'].str.contains('deut'), 'location_country'] = 'germany'\n",
    "users.loc[users['location_country'].str.contains('germ'), 'location_country'] = 'germany'\n",
    "users.loc[users['location_country'].str.contains('berlin'), 'location_country'] = 'germany'\n",
    "users.loc[users['location_country'].str.contains('niedersachsen'), 'location_country'] = 'germany'\n",
    "# united kingdom\n",
    "uk_repls = [\n",
    "    'unitedkingdom', 'eng', 'king', 'wales', 'scotland', 'aberdeenshire', 'camden', 'unitedkindgonm',\n",
    "    'middlesex', 'nottinghamshire', 'westyorkshire', 'cambridgeshire', 'sthelena', 'northyorkshire',\n",
    "    'obviously'\n",
    "]\n",
    "for keyword in uk_repls:\n",
    "    users.loc[users['location_country'].str.contains(keyword), 'location_country'] = 'united kingdom'\n",
    "users.loc[users['location_country'] == 'uk', 'location_country'] = 'united kingdom'\n",
    "# ireland\n",
    "users.loc[users['location_country'].str.contains('countycork'), 'location_country'] = 'ireland'\n",
    "users.loc[users['location_country'].str.contains('cocarlow'), 'location_country'] = 'ireland'\n",
    "# france\n",
    "users.loc[users['location_country'].str.contains('fran'), 'location_country'] = 'france'\n",
    "users.loc[users['location_country'].str.contains('paris'), 'location_country'] = 'france'\n",
    "# spain\n",
    "spain_repl = [\n",
    "    'esp', 'catal', 'galiza', 'euskalherria', 'lleida', 'gipuzkoa', 'orense', 'pontevedra', 'almera',\n",
    "    'bergued', 'andalucia'\n",
    "]\n",
    "for keyword in spain_repl:\n",
    "    users.loc[users['location_country'].str.contains(keyword), 'location_country'] = 'spain'\n",
    "# portugal\n",
    "users.loc[users['location_country'].str.contains('oeiras'), 'location_country'] = 'portugal'\n",
    "# belgium\n",
    "users.loc[users['location_country'].str.contains('labelgique'), 'location_country'] = 'belgium'\n",
    "# austria\n",
    "users.loc[users['location_country'].str.contains('eu'), 'location_country'] = 'austria'\n",
    "# swiss\n",
    "users.loc[users['location_country'].str.contains('lasuisse'), 'location_country'] = 'switzerland'\n",
    "# finland\n",
    "users.loc[users['location_country'].str.contains('etelsuomi'), 'location_country'] = 'finland'\n",
    "# usa\n",
    "usa_repl = [\n",
    "    'unitedstaes', 'america', 'usa', 'state', 'sate', 'cali', 'dc', 'oregon', 'texas', 'florida',\n",
    "    'newhampshire', 'newmexico', 'newjersey', 'newyork', 'virginia', 'bermuda', 'illinois', 'michigan',\n",
    "    'arizona', 'indiana', 'minnesota', 'tennessee', 'dakota', 'connecticut', 'wisconsin', 'ohio',\n",
    "    'maryland', 'northcarolina', 'massachusetts', 'colorado', 'washington', 'maine', 'georgia', 'oklahoma',\n",
    "    'maracopa', 'districtofcolumbia', 'saintloius', 'orangeco', 'aroostook', 'arkansas', 'montana',\n",
    "    'rhodeisland', 'nevada', 'kern', 'fortbend', 'nebraska', 'usofa', 'alabama', 'csa', 'polk',\n",
    "    'alachua', 'austin', 'alaska', 'hawaii', 'worcester', 'iowa', 'cherokee', 'shelby', 'stthomasi',\n",
    "    'vanwert', 'kansas', 'idaho', 'tn', 'framingham', 'pender', 'ysa', 'arizona', 'morgan', 'rutherford'\n",
    "]\n",
    "for keyword in usa_repl:\n",
    "    users.loc[users['location_country'].str.contains(keyword), 'location_country'] = 'usa'\n",
    "users.loc[users['location_country'] == 'us', 'location_country'] = 'usa'\n",
    "users.loc[users['location_country'] == 'ca', 'location_country'] = 'usa'\n",
    "users.loc[users['location_country'] == 'il', 'location_country'] = 'usa'\n",
    "users.loc[users['location_country'] == 'ua', 'location_country'] = 'usa'\n",
    "# cananda\n",
    "canada_repl = [\n",
    "    'cananda', 'british', 'newfoundland', 'newbrunswick', 'alberta', 'ontario', 'lkjlj', 'bc',\n",
    "    'novascotia', 'kcb', 'quebec', 'maricopa', 'travelling', 'vvh', 'saskatchewan'\n",
    "]\n",
    "for keyword in canada_repl:\n",
    "    users.loc[users['location_country'].str.contains(keyword), 'location_country'] = 'canada'\n",
    "# new zealand\n",
    "users.loc[users['location_country'] == 'nz', 'location_country'] = 'newzealand'\n",
    "users.loc[users['location_country'].str.contains('otago'), 'location_country'] = 'newzealand'\n",
    "users.loc[users['location_country'].str.contains('auckland'), 'location_country'] = 'newzealand'\n",
    "# malaysia\n",
    "users.loc[users['location_country'].str.contains('kedah'), 'location_country'] = 'malaysia'\n",
    "# uae\n",
    "users.loc[users['location_country'].str.contains('uae'), 'location_country'] = 'unitedarabemirates'\n",
    "# kuwait\n",
    "users.loc[users['location_country'].str.contains('quit'), 'location_country'] = 'kuwait'\n",
    "# phillipines\n",
    "users.loc[users['location_country'].str.contains('phill'), 'location_country'] = 'philippines'\n",
    "users.loc[users['location_country'].str.contains('metromanila'), 'location_country'] = 'philippines'\n",
    "# uruguay\n",
    "users.loc[users['location_country'].str.contains('urugua'), 'location_country'] = 'uruguay'\n",
    "# panama\n",
    "users.loc[users['location_country'].str.contains('republicofpanama'), 'location_country'] = 'panama'\n",
    "# trinidadandtobago\n",
    "users.loc[users['location_country'].str.contains('westindies'), 'location_country'] = 'trinidadandtobago'\n",
    "# guernsey\n",
    "users.loc[users['location_country'].str.contains('alderney'), 'location_country'] = 'guernsey'\n",
    "# japan\n",
    "users.loc[users['location_country'].str.contains('okinawa'), 'location_country'] = 'japan'\n",
    "# korea\n",
    "users.loc[users['location_country'].str.contains('seoul'), 'location_country'] = 'southkorea'\n",
    "# brazil\n",
    "users.loc[users['location_country'].str.contains('disritofederal'), 'location_country'] = 'brazil'\n",
    "'''\n",
    "location_state\n",
    "'''\n",
    "# usa\n",
    "usa_state_repl = [\n",
    "    'usa', 'texas', 'tx', 'california', 'massachusetts', 'michigan', 'carolina', 'florida', 'colorado', 'pennsylvania',\n",
    "    'newyork', 'newjersey', 'virginia', 'dc', 'washington', 'iowa', 'illinois', 'georgia', 'kansas', 'missouri',\n",
    "    'mississippi', 'oregon', 'arizona', 'ohio', 'tennessee', 'idaho', 'alaska', 'alabama', 'minnesota', 'utah',\n",
    "    'kentucky', 'rhodeisland', 'maryland', 'louisiana', 'indiana', 'connecticut', 'wisconsin', 'newhampshire',\n",
    "    'nevada', 'oklahoma', 'georgia', 'maine', 'newmexico', 'nebraska', 'wyoming', 'frenchquarter', 'fl', 'nebr', 'ct',\n",
    "\n",
    "]\n",
    "for keyword in usa_state_repl:\n",
    "    users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains(keyword)), 'location_country'] = 'usa'\n",
    "# canada\n",
    "canada_state_repl = [\n",
    "    'britishcolumbia', 'newbrunswick', 'novascotia', 'ontario', 'alberta', 'quebec', 'saskatchewan',\n",
    "    'manitoba', \n",
    "]\n",
    "for keyword in canada_state_repl:\n",
    "    users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains(keyword)), 'location_country'] = 'canada'\n",
    "# mexico\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('jalisco')), 'location_country'] = 'mexico'\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('michoacan')), 'location_country'] = 'mexico'\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('morelos')), 'location_country'] = 'mexico'\n",
    "# united kingdom\n",
    "uk_state_repl = [\n",
    "    'newhampshire', 'nottinghamshire', 'england', 'middlesex', 'midlothian', 'scotland', 'westyorkshire',\n",
    "    'canterbury', 'wiltshire', 'kent', 'london', 'cambs', 'herts', 'isleofman', 'surrey', 'cheshire',\n",
    "    'gloucestershire', 'aberdeenshire'\n",
    "]\n",
    "for keyword in uk_state_repl:\n",
    "    users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains(keyword)), 'location_country'] = 'united kingdom'\n",
    "# ireland\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('dublin')), 'location_country'] = 'ireland'\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('wicklow')), 'location_country'] = 'ireland'\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('colimerick')), 'location_country'] = 'ireland'\n",
    "# australia\n",
    "australia_state_repl = [\n",
    "    'newsouthwales', 'victoria', 'australiancapitalterritory', 'southaustralia', 'nsw'\n",
    "]\n",
    "for keyword in australia_state_repl:\n",
    "    users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains(keyword)), 'location_country'] = 'australia'\n",
    "# germany\n",
    "germany_state_repl = [\n",
    "    'nordrheinwestfalen', 'bayern', 'hamburg', 'badenwuerttemberg', 'badenwrttemberg', 'sachsen', 'berlin',\n",
    "    'stuttgart', 'nrw', 'bavaria', 'bremen'\n",
    "]\n",
    "for keyword in germany_state_repl:\n",
    "    users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains(keyword)), 'location_country'] = 'germany'\n",
    "# switzerland\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('bern')), 'location_country'] = 'switzerland'\n",
    "# austria\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('niederoesterreich')), 'location_country'] = 'austria'\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('vienna')), 'location_country'] = 'austria'\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('wien')), 'location_country'] = 'austria'\n",
    "# slovenia\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('ljubljanskaregija')), 'location_country'] = 'slovenia'\n",
    "# spain\n",
    "spain_state_repl = [\n",
    "    'catalunya', 'pontevedra', 'madrid', 'bizkaia', 'asturias', 'pontevedra', 'barcelona', 'pasvasco',\n",
    "    'espaa', 'badajoz', 'gipuzkoa', 'valencia', 'galicia'\n",
    "]\n",
    "for keyword in spain_state_repl:\n",
    "    users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains(keyword)), 'location_country'] = 'spain'\n",
    "# portugal\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('lisboa')), 'location_country'] = 'portugal'\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('coimbra')), 'location_country'] = 'portugal'\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('porto')), 'location_country'] = 'portugal'\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('estremadura')), 'location_country'] = 'portugal'\n",
    "# netherlands\n",
    "netherlands_state_repl = [\n",
    "    'noordholland', 'utrecht', 'zuidholland', 'overijssel', 'friesland', 'northholland', 'schleswigholstein',\n",
    "    'zh', 'twente', \n",
    "]\n",
    "for keyword in netherlands_state_repl:\n",
    "    users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains(keyword)), 'location_country'] = 'netherlands'\n",
    "# belgium\n",
    "belgium_state_repl = [\n",
    "    'vlaamsbrabant', 'liege'\n",
    "]\n",
    "for keyword in belgium_state_repl:\n",
    "    users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains(keyword)), 'location_country'] = 'belgium'\n",
    "# new zealand\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('auckland')), 'location_country'] = 'newzealand'\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('northisland')), 'location_country'] = 'newzealand'\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('waikato')), 'location_country'] = 'newzealand'\n",
    "# italy\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('italia')), 'location_country'] = 'italy'\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('toscana')), 'location_country'] = 'italy'\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('piemonte')), 'location_country'] = 'italy'\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('lombardia')), 'location_country'] = 'italy'\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('gorizia')), 'location_country'] = 'italy'\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'] == 're'), 'location_country'] = 'italy'\n",
    "# greece\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('townofbali')), 'location_country'] = 'greece'\n",
    "# nigeria\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('imostate')), 'location_country'] = 'nigeria'\n",
    "# southafrica\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('westerncape')), 'location_country'] = 'southafrica'\n",
    "# romania\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('ilfov')), 'location_country'] = 'romania'\n",
    "# malaysia\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('penang')), 'location_country'] = 'malaysia'\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('negerisembilan')), 'location_country'] = 'malaysia'\n",
    "# indonesia\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('jakarta')), 'location_country'] = 'indonesia'\n",
    "# philippines\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('laguna')), 'location_country'] = 'philippines'\n",
    "# singapore\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('singapore')), 'location_country'] = 'singapore'\n",
    "# pakistan\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('punjab')), 'location_country'] = 'pakistan'\n",
    "# denmark\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('jutland')), 'location_country'] = 'denmark'\n",
    "# france\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('lorraine')), 'location_country'] = 'france'\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('hautegaronne')), 'location_country'] = 'france'\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('heraut')), 'location_country'] = 'france'\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('rhnealpes')), 'location_country'] = 'france'\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('iledefrance')), 'location_country'] = 'france'\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('paca')), 'location_country'] = 'france'\n",
    "# uruguay\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('montevideo')), 'location_country'] = 'uruguay'\n",
    "# argentina\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('buenosaires')), 'location_country'] = 'argentina'\n",
    "# peru\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('southamerica')), 'location_country'] = 'peru'\n",
    "# chile\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('santiago')), 'location_country'] = 'chile'\n",
    "# japan\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('iwakuni')), 'location_country'] = 'japan'\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_state'].str.contains('tokyo')), 'location_country'] = 'japan'\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "location_city\n",
    "'''\n",
    "# usa\n",
    "usa_city_repl = [\n",
    "    'losang', 'seattle', 'sanf', 'sand', 'newyork', 'newark', 'newbedford', 'portland', 'cincinnati',\n",
    "    'houston', 'albuquerque', 'chicago', 'austin', 'beaverton', 'raleigh', 'richmond', 'fairbanks',\n",
    "    'minneapolis', 'stlouis', 'tucson', 'oakland', 'boston', 'kansascity', 'denver', 'springfield',\n",
    "    'topeka', 'dallas', 'asheville', 'buffalo', 'fremont', 'stpaul', 'elcajon', 'miami', 'marysville',\n",
    "    'baltimore', 'charleston', 'santamonica', 'knoxville', 'rochester', 'orlando', 'coloradosprings',\n",
    "    'arlington', 'pensacola', 'sanjose', 'cedarrapids', 'olympia', 'lasvegas', 'mercerisland',\n",
    "    'encinitas', 'omaha', 'lawrence', 'sacramento', 'norfolk', 'kirkwood', 'tallahassee', 'lexington',\n",
    "    'kalamazoo', 'orleans', 'desmoines', 'aurora', 'annarbor', 'newbern', 'somerville', 'lakeland',\n",
    "    'hartford', 'tigard', 'phoenix', 'irvine', 'sanantonio', 'mesa', 'brooklyn', 'philadelphia',\n",
    "    'lacey', 'greenbay', 'pittsburg', 'wichita', 'elizabeth', 'murrieta', 'batonrouge', 'yuma',\n",
    "    'baycity', 'lynchburg', 'santabarbara', 'statenisland', 'saintpaul', 'lakewood', 'fallschurch',\n",
    "    'northhaven', 'frederick', 'milwaukie', 'cary', 'stcharles', 'lewiston', 'virginiabeach',\n",
    "    'longbranch', 'indianapolis', 'portales', 'fountainvalley', 'sebastopol', 'washington', 'louisville',\n",
    "    'millersburg'\n",
    "]\n",
    "for keyword in usa_city_repl:\n",
    "    users.loc[(users['location_country'] == 'null') & (users['location_city'].str.contains(keyword)), 'location_country'] = 'usa'\n",
    "# canada\n",
    "canada_city_repl = [\n",
    "    'calgary', 'vancouver', 'toronto', 'ottawa', 'fredericton', 'victoria', 'hamilton', 'montreal',\n",
    "    'kelowna', 'winnipeg', 'saskatoon', 'halifax', 'edmonton', 'kitchener', 'regina', 'lethbridge',\n",
    "\n",
    "]\n",
    "for keyword in canada_city_repl:\n",
    "    users.loc[(users['location_country'] == 'null') & (users['location_city'].str.contains(keyword)), 'location_country'] = 'canada'\n",
    "# italy\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_city'].str.contains('milano')), 'location_country'] = 'italy'\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_city'].str.contains('roma')), 'location_country'] = 'italy'\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_city'].str.contains('rome')), 'location_country'] = 'italy'\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_city'].str.contains('genova')), 'location_country'] = 'italy'\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_city'].str.contains('torino')), 'location_country'] = 'italy'\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_city'].str.contains('perugia')), 'location_country'] = 'italy'\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_city'].str.contains('salerno')), 'location_country'] = 'italy'\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_city'].str.contains('firenze')), 'location_country'] = 'italy'\n",
    "# united kingdom\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_city'].str.contains('london')), 'location_country'] = 'united kingdom'\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_city'].str.contains('manchester')), 'location_country'] = 'united kingdom'\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_city'].str.contains('cambridge')), 'location_country'] = 'united kingdom'\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_city'].str.contains('york')), 'location_country'] = 'united kingdom'\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_city'].str.contains('birmingham')), 'location_country'] = 'united kingdom'\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_city'].str.contains('edinburgh')), 'location_country'] = 'united kingdom'\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_city'].str.contains('newcastle')), 'location_country'] = 'united kingdom'\n",
    "# germany\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_city'].str.contains('hamburg')), 'location_country'] = 'germany'\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_city'].str.contains('berlin')), 'location_country'] = 'germany'\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_city'].str.contains('augsburg')), 'location_country'] = 'germany'\n",
    "# france\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_city'].str.contains('paris')), 'location_country'] = 'france'\n",
    "# spain\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_city'].str.contains('barcelona')), 'location_country'] = 'spain'\n",
    "# finland\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_city'].str.contains('helsinki')), 'location_country'] = 'finland'\n",
    "# australia\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_city'].str.contains('melbourne')), 'location_country'] = 'australia'\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_city'].str.contains('sidney')), 'location_country'] = 'australia'\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_city'].str.contains('canberra')), 'location_country'] = 'australia'\n",
    "# singapore\n",
    "users.loc[(users['location_country'] == 'null') & (users['location_city'].str.contains('singapore')), 'location_country'] = 'singapore'\n",
    "\n",
    "'''\n",
    "모든 null 값을 다 볼 순 없으므로,\n",
    "user_id 별 데이터 많은 순서대로 null 값 처리\n",
    "'''\n",
    "# usa\n",
    "usa_uids = [\n",
    "    83671, 179718, 187065, 104278, 146230, 93565, 67663, 84795, 175100,\n",
    "    273190, 51350, 19493, 226745, 57620, 125031, 113663, 178201, 91631,\n",
    "    83443, 239535, 135228, 23680, 259264, 209229, 929, 168036, 50129,\n",
    "    129368, 136465, 8937, 84523, 241749, 48743, 132188, 270897, 171045,\n",
    "    44842, 115473, 1131, 91017, 68768, 167587, 135411, 30889, 221557,\n",
    "    39195, 154346, 273110, 29497, 223816, 38718, 175529, 186238, 239449,\n",
    "    141543, 77676, 258277, 240113, 172486, 34988, 112818, 129474, 46295,\n",
    "    142041, 268035, 176102, 126985, 93386, 114601, 30650, 24105, 170850,\n",
    "    28372, 207651, 122802, 129389, 266764, 269140, 50504, 52993, 170208,\n",
    "    162264, 45641, 226556, 241214\n",
    "\n",
    "]\n",
    "for uid in usa_uids:\n",
    "    users.loc[users['user_id'] == uid, 'location_country'] = 'usa'    \n",
    "# uk\n",
    "users.loc[users['user_id'] == 178522, 'location_country'] = 'united kingdom'\n",
    "users.loc[users['user_id'] == 5476, 'location_country'] = 'united kingdom'\n",
    "users.loc[users['user_id'] == 237064, 'location_country'] = 'united kingdom'\n",
    "users.loc[users['user_id'] == 241537, 'location_country'] = 'united kingdom'\n",
    "# ireland\n",
    "users.loc[users['user_id'] == 26432, 'location_country'] = 'ireland'\n",
    "# canada\n",
    "canada_uids = [44089, 79188, 176100, 34087, 172962, 103160, 206693]\n",
    "for uid in canada_uids:\n",
    "    users.loc[users['user_id'] == uid, 'location_country'] = 'canada'\n",
    "# france\n",
    "users.loc[users['user_id'] == 179641, 'location_country'] = 'france'\n",
    "# germany\n",
    "users.loc[users['user_id'] == 276538, 'location_country'] = 'germany'\n",
    "users.loc[users['user_id'] == 102169, 'location_country'] = 'germany'\n",
    "# austria\n",
    "users.loc[users['user_id'] == 3923, 'location_country'] = 'austria'\n",
    "users.loc[users['user_id'] == 14393, 'location_country'] = 'austria'\n",
    "# portugal\n",
    "users.loc[users['user_id'] == 164581, 'location_country'] = 'portugal'\n",
    "# australia\n",
    "users.loc[users['user_id'] == 11399, 'location_country'] = 'australia'\n",
    "# malaysia\n",
    "users.loc[users['user_id'] == 30445, 'location_country'] = 'malaysia'\n",
    "users.loc[users['user_id'] == 28543, 'location_country'] = 'malaysia'\n",
    "# philippines\n",
    "users.loc[users['user_id'] == 131023, 'location_country'] = 'philippines'\n",
    "#########################\n",
    "#########################\n",
    "users = users.drop(['location_city', 'location_state'], axis=1)\n",
    "#########################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#출판사\n",
    "publisher_dict=(books['publisher'].value_counts()).to_dict()\n",
    "publisher_count_df= pd.DataFrame(list(publisher_dict.items()),columns = ['publisher','count'])\n",
    "publisher_count_df = publisher_count_df.sort_values(by=['count'], ascending = False)\n",
    "modify_list = publisher_count_df[publisher_count_df['count']>1].publisher.values\n",
    "for publisher in modify_list:\n",
    "    try:\n",
    "        number = books[books['publisher']==publisher]['isbn'].apply(lambda x: x[:4]).value_counts().index[0]\n",
    "        right_publisher = books[books['isbn'].apply(lambda x: x[:4])==number]['publisher'].value_counts().index[0]\n",
    "        books.loc[books[books['isbn'].apply(lambda x: x[:4])==number].index,'publisher'] = right_publisher\n",
    "    except: \n",
    "        pass\n",
    "#카테고리\n",
    "books.loc[books[books['category'].notnull()].index, 'category'] = books[books['category'].notnull()]['category'].apply(lambda x: re.sub('[\\W_]+',' ',x).strip())\n",
    "category_df = pd.DataFrame(books['category'].value_counts()).reset_index()\n",
    "category_df.columns = ['category','count']  \n",
    "books['category_high'] = books['category'].copy()\n",
    "books.loc[books[books['category']=='biography'].index, 'category_high'] = 'biography autobiography'\n",
    "books.loc[books[books['category']=='autobiography'].index,'category_high'] = 'biography autobiography'\n",
    "books.loc[books[books['category'].str.contains('history',na=False)].index,'category_high'] = 'history'\n",
    "categories = ['garden','crafts','physics','adventure','music','fiction','nonfiction','science','science fiction','social','homicide',\n",
    "'sociology','disease','religion','christian','philosophy','psycholog','mathemat','agricult','environmental',\n",
    "'business','poetry','drama','literary','travel','motion picture','children','cook','literature','electronic',\n",
    "'humor','animal','bird','photograph','computer','house','ecology','family','architect','camp','criminal','language','india']\n",
    "for category in categories:\n",
    "    books.loc[books[books['category'].str.contains(category,na=False)].index,'category_high'] = category\n",
    "category_high_df = pd.DataFrame(books['category_high'].value_counts()).reset_index()\n",
    "category_high_df.columns = ['category','count']\n",
    "# 5개 이하인 항목은 others로 묶어주도록 하겠습니다.\n",
    "others_list = category_high_df[category_high_df['count']<5]['category'].values\n",
    "books.loc[books[books['category_high'].isin(others_list)].index, 'category_high']='others'\n",
    "\n",
    "\n",
    "# # del books['category']\n",
    "# # books.rename(columns = {'category_high':'category'},inplace=True)\n",
    "# # 인덱싱 처리된 데이터 조인\n",
    "# # isbn,book_title,book_author,year_of_publication,publisher,img_url,language,category,summary,img_path\n",
    "# ratings = pd.concat([ratings1, ratings2]).reset_index(drop=True)\n",
    "# context_df = ratings.merge(users, on='user_id', how='left').merge(books[['isbn', 'category', 'publisher', 'language', 'book_author']], on='isbn', how='left')\n",
    "# train_df = ratings1.merge(users, on='user_id', how='left').merge(books[['isbn', 'category', 'publisher', 'language', 'book_author']], on='isbn', how='left')\n",
    "# test_df = ratings2.merge(users, on='user_id', how='left').merge(books[['isbn', 'category', 'publisher', 'language', 'book_author']], on='isbn', how='left')\n",
    "# # 인덱싱 처리\n",
    "# # loc_city2idx = {v:k for k,v in enumerate(context_df['location_city'].unique())}\n",
    "# # loc_state2idx = {v:k for k,v in enumerate(context_df['location_state'].unique())}\n",
    "# loc_country2idx = {v:k for k,v in enumerate(context_df['location_country'].unique())}\n",
    "# # train_df['location_city'] = train_df['location_city'].map(loc_city2idx)\n",
    "# # train_df['location_state'] = train_df['location_state'].map(loc_state2idx)\n",
    "# train_df['location_country'] = train_df['location_country'].map(loc_country2idx)\n",
    "# # test_df['location_city'] = test_df['location_city'].map(loc_city2idx)\n",
    "# # test_df['location_state'] = test_df['location_state'].map(loc_state2idx)\n",
    "# test_df['location_country'] = test_df['location_country'].map(loc_country2idx)\n",
    "# train_df['age'] = train_df['age'].fillna(int(train_df['age'].mean()))\n",
    "# train_df['age'] = train_df['age'].apply(age_map)\n",
    "# test_df['age'] = test_df['age'].fillna(int(test_df['age'].mean()))\n",
    "# test_df['age'] = test_df['age'].apply(age_map)\n",
    "# # book 파트 인덱싱\n",
    "# category2idx = {v:k for k,v in enumerate(context_df['category'].unique())}\n",
    "# publisher2idx = {v:k for k,v in enumerate(context_df['publisher'].unique())}\n",
    "# language2idx = {v:k for k,v in enumerate(context_df['language'].unique())}\n",
    "# author2idx = {v:k for k,v in enumerate(context_df['book_author'].unique())}\n",
    "# train_df['category'] = train_df['category'].map(category2idx)\n",
    "# train_df['publisher'] = train_df['publisher'].map(publisher2idx)\n",
    "# train_df['language'] = train_df['language'].map(language2idx)\n",
    "# train_df['book_author'] = train_df['book_author'].map(author2idx)\n",
    "# test_df['category'] = test_df['category'].map(category2idx)\n",
    "# test_df['publisher'] = test_df['publisher'].map(publisher2idx)\n",
    "# test_df['language'] = test_df['language'].map(language2idx)\n",
    "# test_df['book_author'] = test_df['book_author'].map(author2idx)\n",
    "# idx = {\n",
    "#     # \"loc_city2idx\":loc_city2idx,\n",
    "#     # \"loc_state2idx\":loc_state2idx,\n",
    "#     \"loc_country2idx\":loc_country2idx,\n",
    "#     \"category2idx\":category2idx,\n",
    "#     \"publisher2idx\":publisher2idx,\n",
    "#     \"language2idx\":language2idx,\n",
    "#     \"author2idx\":author2idx,\n",
    "# }\n",
    "# # return idx, train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>age</th>\n",
       "      <th>location_country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11400</td>\n",
       "      <td>49.0</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11676</td>\n",
       "      <td>NaN</td>\n",
       "      <td>null</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>67544</td>\n",
       "      <td>30.0</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>85526</td>\n",
       "      <td>36.0</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>96054</td>\n",
       "      <td>29.0</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>116866</td>\n",
       "      <td>NaN</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>123629</td>\n",
       "      <td>NaN</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>177458</td>\n",
       "      <td>29.0</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>200273</td>\n",
       "      <td>NaN</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id   age location_country\n",
       "0        8   NaN           canada\n",
       "1    11400  49.0           canada\n",
       "2    11676   NaN             null\n",
       "3    67544  30.0           canada\n",
       "4    85526  36.0           canada\n",
       "5    96054  29.0           canada\n",
       "6   116866   NaN           canada\n",
       "7   123629   NaN           canada\n",
       "8   177458  29.0           canada\n",
       "9   200273   NaN           canada"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isbn</th>\n",
       "      <th>book_title</th>\n",
       "      <th>book_author</th>\n",
       "      <th>year_of_publication</th>\n",
       "      <th>publisher</th>\n",
       "      <th>img_url</th>\n",
       "      <th>language</th>\n",
       "      <th>category</th>\n",
       "      <th>summary</th>\n",
       "      <th>img_path</th>\n",
       "      <th>category_high</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0002005018</td>\n",
       "      <td>Clara Callan</td>\n",
       "      <td>Richard Bruce Wright</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>Collins</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.0...</td>\n",
       "      <td>en</td>\n",
       "      <td>Actresses</td>\n",
       "      <td>In a small town in Canada, Clara Callan reluct...</td>\n",
       "      <td>images/0002005018.01.THUMBZZZ.jpg</td>\n",
       "      <td>Actresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0060973129</td>\n",
       "      <td>Decision in Normandy</td>\n",
       "      <td>Carlo D'Este</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>Perennial</td>\n",
       "      <td>http://images.amazon.com/images/P/0060973129.0...</td>\n",
       "      <td>en</td>\n",
       "      <td>1940 1949</td>\n",
       "      <td>Here, for the first time in paperback, is an o...</td>\n",
       "      <td>images/0060973129.01.THUMBZZZ.jpg</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0374157065</td>\n",
       "      <td>Flu: The Story of the Great Influenza Pandemic...</td>\n",
       "      <td>Gina Bari Kolata</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>Farrar Straus Giroux</td>\n",
       "      <td>http://images.amazon.com/images/P/0374157065.0...</td>\n",
       "      <td>en</td>\n",
       "      <td>Medical</td>\n",
       "      <td>Describes the great flu epidemic of 1918, an o...</td>\n",
       "      <td>images/0374157065.01.THUMBZZZ.jpg</td>\n",
       "      <td>Medical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0399135782</td>\n",
       "      <td>The Kitchen God's Wife</td>\n",
       "      <td>Amy Tan</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>Putnam Pub Group</td>\n",
       "      <td>http://images.amazon.com/images/P/0399135782.0...</td>\n",
       "      <td>en</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>A Chinese immigrant who is convinced she is dy...</td>\n",
       "      <td>images/0399135782.01.THUMBZZZ.jpg</td>\n",
       "      <td>Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0425176428</td>\n",
       "      <td>What If?: The World's Foremost Military Histor...</td>\n",
       "      <td>Robert Cowley</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>Berkley Publishing Group</td>\n",
       "      <td>http://images.amazon.com/images/P/0425176428.0...</td>\n",
       "      <td>en</td>\n",
       "      <td>History</td>\n",
       "      <td>Essays by respected military historians, inclu...</td>\n",
       "      <td>images/0425176428.01.THUMBZZZ.jpg</td>\n",
       "      <td>History</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0671870432</td>\n",
       "      <td>PLEADING GUILTY</td>\n",
       "      <td>Scott Turow</td>\n",
       "      <td>1993.0</td>\n",
       "      <td>Pocket</td>\n",
       "      <td>http://images.amazon.com/images/P/0671870432.0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>images/0671870432.01.THUMBZZZ.jpg</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>074322678X</td>\n",
       "      <td>Where You'll Find Me: And Other Stories</td>\n",
       "      <td>Ann Beattie</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>Pocket</td>\n",
       "      <td>http://images.amazon.com/images/P/074322678X.0...</td>\n",
       "      <td>en</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Now back in print, Ann Beattie&amp;#39;s finest sh...</td>\n",
       "      <td>images/074322678X.01.THUMBZZZ.jpg</td>\n",
       "      <td>Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0771074670</td>\n",
       "      <td>Nights Below Station Street</td>\n",
       "      <td>David Adams Richards</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>McClelland &amp; Stewart</td>\n",
       "      <td>http://images.amazon.com/images/P/0771074670.0...</td>\n",
       "      <td>en</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>Another story based in the fictional rural tow...</td>\n",
       "      <td>images/0771074670.01.THUMBZZZ.jpg</td>\n",
       "      <td>Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0887841740</td>\n",
       "      <td>The Middle Stories</td>\n",
       "      <td>Sheila Heti</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>HarperBusiness</td>\n",
       "      <td>http://images.amazon.com/images/P/0887841740.0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>images/0887841740.01.THUMBZZZ.jpg</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1552041778</td>\n",
       "      <td>Jane Doe</td>\n",
       "      <td>R. J. Kaiser</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>Firefly Books Ltd</td>\n",
       "      <td>http://images.amazon.com/images/P/1552041778.0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>images/1552041778.01.THUMBZZZ.jpg</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         isbn                                         book_title  \\\n",
       "0  0002005018                                       Clara Callan   \n",
       "1  0060973129                               Decision in Normandy   \n",
       "2  0374157065  Flu: The Story of the Great Influenza Pandemic...   \n",
       "3  0399135782                             The Kitchen God's Wife   \n",
       "4  0425176428  What If?: The World's Foremost Military Histor...   \n",
       "5  0671870432                                    PLEADING GUILTY   \n",
       "6  074322678X            Where You'll Find Me: And Other Stories   \n",
       "7  0771074670                        Nights Below Station Street   \n",
       "8  0887841740                                 The Middle Stories   \n",
       "9  1552041778                                           Jane Doe   \n",
       "\n",
       "            book_author  year_of_publication                 publisher  \\\n",
       "0  Richard Bruce Wright               2001.0                   Collins   \n",
       "1          Carlo D'Este               1991.0                 Perennial   \n",
       "2      Gina Bari Kolata               1999.0      Farrar Straus Giroux   \n",
       "3               Amy Tan               1991.0          Putnam Pub Group   \n",
       "4         Robert Cowley               2000.0  Berkley Publishing Group   \n",
       "5           Scott Turow               1993.0                    Pocket   \n",
       "6           Ann Beattie               2002.0                    Pocket   \n",
       "7  David Adams Richards               1988.0      McClelland & Stewart   \n",
       "8           Sheila Heti               2004.0            HarperBusiness   \n",
       "9          R. J. Kaiser               1999.0         Firefly Books Ltd   \n",
       "\n",
       "                                             img_url language   category  \\\n",
       "0  http://images.amazon.com/images/P/0002005018.0...       en  Actresses   \n",
       "1  http://images.amazon.com/images/P/0060973129.0...       en  1940 1949   \n",
       "2  http://images.amazon.com/images/P/0374157065.0...       en    Medical   \n",
       "3  http://images.amazon.com/images/P/0399135782.0...       en    Fiction   \n",
       "4  http://images.amazon.com/images/P/0425176428.0...       en    History   \n",
       "5  http://images.amazon.com/images/P/0671870432.0...      NaN        NaN   \n",
       "6  http://images.amazon.com/images/P/074322678X.0...       en    Fiction   \n",
       "7  http://images.amazon.com/images/P/0771074670.0...       en    Fiction   \n",
       "8  http://images.amazon.com/images/P/0887841740.0...      NaN        NaN   \n",
       "9  http://images.amazon.com/images/P/1552041778.0...      NaN        NaN   \n",
       "\n",
       "                                             summary  \\\n",
       "0  In a small town in Canada, Clara Callan reluct...   \n",
       "1  Here, for the first time in paperback, is an o...   \n",
       "2  Describes the great flu epidemic of 1918, an o...   \n",
       "3  A Chinese immigrant who is convinced she is dy...   \n",
       "4  Essays by respected military historians, inclu...   \n",
       "5                                                NaN   \n",
       "6  Now back in print, Ann Beattie&#39;s finest sh...   \n",
       "7  Another story based in the fictional rural tow...   \n",
       "8                                                NaN   \n",
       "9                                                NaN   \n",
       "\n",
       "                            img_path category_high  \n",
       "0  images/0002005018.01.THUMBZZZ.jpg     Actresses  \n",
       "1  images/0060973129.01.THUMBZZZ.jpg        others  \n",
       "2  images/0374157065.01.THUMBZZZ.jpg       Medical  \n",
       "3  images/0399135782.01.THUMBZZZ.jpg       Fiction  \n",
       "4  images/0425176428.01.THUMBZZZ.jpg       History  \n",
       "5  images/0671870432.01.THUMBZZZ.jpg           NaN  \n",
       "6  images/074322678X.01.THUMBZZZ.jpg       Fiction  \n",
       "7  images/0771074670.01.THUMBZZZ.jpg       Fiction  \n",
       "8  images/0887841740.01.THUMBZZZ.jpg           NaN  \n",
       "9  images/1552041778.01.THUMBZZZ.jpg           NaN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "books.to_csv('./data/newbooks.csv', index=False)\n",
    "users.to_csv('./data/newusers.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
