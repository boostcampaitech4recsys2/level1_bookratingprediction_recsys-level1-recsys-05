{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc77a5f3-8a4a-4819-b39f-f394575017a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import tqdm\n",
    "import pdb\n",
    "from scipy.sparse import csr_matrix, linalg\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2f4cb31-3354-4f9f-ac2d-b4ff0c41739c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a881b1b2-a823-42a3-82ef-65c883c8438c",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = pd.read_csv(\"../data/books.csv\")\n",
    "train = pd.read_csv(\"../data/train_ratings.csv\")\n",
    "test = pd.read_csv(\"../data/test_ratings.csv\")\n",
    "users = pd.read_csv(\"../data/users.csv\")\n",
    "sub = pd.read_csv(\"../data/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "444f6e59-b7ac-4f7e-9fe9-9fd46ee2c028",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = pd.concat([train['user_id'], sub['user_id']]).unique()\n",
    "isbns = pd.concat([train['isbn'], sub['isbn']]).unique()\n",
    "\n",
    "idx2user = {idx:id for idx, id in enumerate(ids)}\n",
    "idx2isbn = {idx:isbn for idx, isbn in enumerate(isbns)}\n",
    "\n",
    "user2idx = {id:idx for idx, id in idx2user.items()}\n",
    "isbn2idx = {isbn:idx for idx, isbn in idx2isbn.items()}\n",
    "\n",
    "train['user_id'] = train['user_id'].map(user2idx)\n",
    "sub['user_id'] = sub['user_id'].map(user2idx)\n",
    "\n",
    "train['isbn'] = train['isbn'].map(isbn2idx)\n",
    "sub['isbn'] = sub['isbn'].map(isbn2idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd3255a3-1ac2-4efe-81cd-c0fa7b42714d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_vector(path):\n",
    "    img = Image.open(path)\n",
    "    scale = transforms.Resize((32, 32))\n",
    "    tensor = transforms.ToTensor()\n",
    "    img_fe = Variable(tensor(scale(img)))\n",
    "    return img_fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d384ab3e-5320-463d-b715-240b2b04e6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         data/images/0002005018.01.THUMBZZZ.jpg\n",
      "1         data/images/0060973129.01.THUMBZZZ.jpg\n",
      "2         data/images/0374157065.01.THUMBZZZ.jpg\n",
      "3         data/images/0399135782.01.THUMBZZZ.jpg\n",
      "4         data/images/0425176428.01.THUMBZZZ.jpg\n",
      "                           ...                  \n",
      "129772    data/images/0743525493.01.THUMBZZZ.jpg\n",
      "129773    data/images/067161746X.01.THUMBZZZ.jpg\n",
      "129774    data/images/0884159221.01.THUMBZZZ.jpg\n",
      "129775    data/images/0912333022.01.THUMBZZZ.jpg\n",
      "129776    data/images/1569661057.01.THUMBZZZ.jpg\n",
      "Name: img_path, Length: 129777, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#img_train 생성\n",
    "books_ = books.copy()\n",
    "books_['isbn'] = books_['isbn'].map(isbn2idx)\n",
    "\n",
    "df = train.copy()\n",
    "train1 = True\n",
    "\n",
    "if train1 == True:\n",
    "    df_ = df.copy()\n",
    "else:\n",
    "    df_ = df.copy()\n",
    "    df_['user_id'] = df_['user_id'].map(user2idx)\n",
    "    df_['isbn'] = df_['isbn'].map(isbn2idx)\n",
    "\n",
    "df_ = pd.merge(df_, books_[['isbn', 'img_path']], on='isbn', how='left')\n",
    "df_['img_path'] = df_['img_path'].apply(lambda x: 'data/'+x)\n",
    "img_vector_df = df_[['img_path']].drop_duplicates().reset_index(drop=True).copy()\n",
    "data_box = []\n",
    "print(img_vector_df['img_path'])\n",
    "for idx, path in enumerate(sorted(img_vector_df['img_path'])):\n",
    "    data = image_vector(\"../\"+path)\n",
    "    if data.size()[0] == 3:\n",
    "        data_box.append(np.array(data))\n",
    "    else:\n",
    "        data_box.append(np.array(data.expand(3, data.size()[1], data.size()[2])))\n",
    "img_vector_df['img_vector'] = data_box\n",
    "df_ = pd.merge(df_, img_vector_df, on='img_path', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4437bddd-222e-4b88-a078-3c7c4ae634d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>isbn</th>\n",
       "      <th>rating</th>\n",
       "      <th>img_path</th>\n",
       "      <th>img_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>data/images/0002005018.01.THUMBZZZ.jpg</td>\n",
       "      <td>[[[0.003921569, 0.003921569, 0.003921569, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>data/images/0002005018.01.THUMBZZZ.jpg</td>\n",
       "      <td>[[[0.003921569, 0.003921569, 0.003921569, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>data/images/0002005018.01.THUMBZZZ.jpg</td>\n",
       "      <td>[[[0.003921569, 0.003921569, 0.003921569, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>data/images/0002005018.01.THUMBZZZ.jpg</td>\n",
       "      <td>[[[0.003921569, 0.003921569, 0.003921569, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>data/images/0002005018.01.THUMBZZZ.jpg</td>\n",
       "      <td>[[[0.003921569, 0.003921569, 0.003921569, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306790</th>\n",
       "      <td>6313</td>\n",
       "      <td>129772</td>\n",
       "      <td>7</td>\n",
       "      <td>data/images/0743525493.01.THUMBZZZ.jpg</td>\n",
       "      <td>[[[0.52156866, 0.53333336, 0.53333336, 0.49803...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306791</th>\n",
       "      <td>1879</td>\n",
       "      <td>129773</td>\n",
       "      <td>6</td>\n",
       "      <td>data/images/067161746X.01.THUMBZZZ.jpg</td>\n",
       "      <td>[[[0.79607844, 0.85882354, 0.80784315, 0.82745...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306792</th>\n",
       "      <td>1879</td>\n",
       "      <td>129774</td>\n",
       "      <td>7</td>\n",
       "      <td>data/images/0884159221.01.THUMBZZZ.jpg</td>\n",
       "      <td>[[[0.8980392, 0.9254902, 0.93333334, 0.9294117...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306793</th>\n",
       "      <td>1879</td>\n",
       "      <td>129775</td>\n",
       "      <td>7</td>\n",
       "      <td>data/images/0912333022.01.THUMBZZZ.jpg</td>\n",
       "      <td>[[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306794</th>\n",
       "      <td>1879</td>\n",
       "      <td>129776</td>\n",
       "      <td>10</td>\n",
       "      <td>data/images/1569661057.01.THUMBZZZ.jpg</td>\n",
       "      <td>[[[0.89411765, 0.9490196, 0.9411765, 0.9490196...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>306795 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id    isbn  rating                                img_path  \\\n",
       "0             0       0       4  data/images/0002005018.01.THUMBZZZ.jpg   \n",
       "1             1       0       7  data/images/0002005018.01.THUMBZZZ.jpg   \n",
       "2             2       0       8  data/images/0002005018.01.THUMBZZZ.jpg   \n",
       "3             3       0       8  data/images/0002005018.01.THUMBZZZ.jpg   \n",
       "4             4       0       9  data/images/0002005018.01.THUMBZZZ.jpg   \n",
       "...         ...     ...     ...                                     ...   \n",
       "306790     6313  129772       7  data/images/0743525493.01.THUMBZZZ.jpg   \n",
       "306791     1879  129773       6  data/images/067161746X.01.THUMBZZZ.jpg   \n",
       "306792     1879  129774       7  data/images/0884159221.01.THUMBZZZ.jpg   \n",
       "306793     1879  129775       7  data/images/0912333022.01.THUMBZZZ.jpg   \n",
       "306794     1879  129776      10  data/images/1569661057.01.THUMBZZZ.jpg   \n",
       "\n",
       "                                               img_vector  \n",
       "0       [[[0.003921569, 0.003921569, 0.003921569, 0.00...  \n",
       "1       [[[0.003921569, 0.003921569, 0.003921569, 0.00...  \n",
       "2       [[[0.003921569, 0.003921569, 0.003921569, 0.00...  \n",
       "3       [[[0.003921569, 0.003921569, 0.003921569, 0.00...  \n",
       "4       [[[0.003921569, 0.003921569, 0.003921569, 0.00...  \n",
       "...                                                   ...  \n",
       "306790  [[[0.52156866, 0.53333336, 0.53333336, 0.49803...  \n",
       "306791  [[[0.79607844, 0.85882354, 0.80784315, 0.82745...  \n",
       "306792  [[[0.8980392, 0.9254902, 0.93333334, 0.9294117...  \n",
       "306793  [[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...  \n",
       "306794  [[[0.89411765, 0.9490196, 0.9411765, 0.9490196...  \n",
       "\n",
       "[306795 rows x 5 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6354ff1e-20eb-4936-b273-973fc04c04ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "52000it [00:21, 2446.80it/s]\n"
     ]
    }
   ],
   "source": [
    "books_ = books.copy()\n",
    "books_['isbn'] = books_['isbn'].map(isbn2idx)\n",
    "\n",
    "df1 = test.copy()\n",
    "\n",
    "# if train == True:\n",
    "#     df_ = df.copy()\n",
    "# else:\n",
    "df_1 = df1.copy()\n",
    "df_1['user_id'] = df_1['user_id'].map(user2idx)\n",
    "df_1['isbn'] = df_1['isbn'].map(isbn2idx)\n",
    "\n",
    "df_1= pd.merge(df_1, books_[['isbn', 'img_path']], on='isbn', how='left')\n",
    "df_1['img_path'] = df_1['img_path'].apply(lambda x: 'data/'+x)\n",
    "img_vector_df = df_1[['img_path']].drop_duplicates().reset_index(drop=True).copy()\n",
    "data_box = []\n",
    "for idx, path in tqdm(enumerate(sorted(img_vector_df['img_path']))):\n",
    "    data = image_vector(\"../\"+path)\n",
    "    if data.size()[0] == 3:\n",
    "        data_box.append(np.array(data))\n",
    "    else:\n",
    "        data_box.append(np.array(data.expand(3, data.size()[1], data.size()[2])))\n",
    "img_vector_df['img_vector'] = data_box\n",
    "df_1 = pd.merge(df_1, img_vector_df, on='img_path', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79d24122-0eb9-4873-9ac7-b0c2a2224ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>isbn</th>\n",
       "      <th>rating</th>\n",
       "      <th>img_path</th>\n",
       "      <th>img_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>data/images/0002005018.01.THUMBZZZ.jpg</td>\n",
       "      <td>[[[0.99215686, 0.99607843, 0.9882353, 0.996078...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13426</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>data/images/0002005018.01.THUMBZZZ.jpg</td>\n",
       "      <td>[[[0.99215686, 0.99607843, 0.9882353, 0.996078...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26761</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>data/images/0060973129.01.THUMBZZZ.jpg</td>\n",
       "      <td>[[[0.40784314, 0.59607846, 0.6313726, 0.411764...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16495</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>data/images/0374157065.01.THUMBZZZ.jpg</td>\n",
       "      <td>[[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6225</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>data/images/0399135782.01.THUMBZZZ.jpg</td>\n",
       "      <td>[[[0.003921569, 0.003921569, 0.003921569, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76694</th>\n",
       "      <td>7728</td>\n",
       "      <td>149565</td>\n",
       "      <td>0</td>\n",
       "      <td>data/images/1576734218.01.THUMBZZZ.jpg</td>\n",
       "      <td>[[[0.003921569, 0.003921569, 0.003921569, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76695</th>\n",
       "      <td>47785</td>\n",
       "      <td>149566</td>\n",
       "      <td>0</td>\n",
       "      <td>data/images/3492223710.01.THUMBZZZ.jpg</td>\n",
       "      <td>[[[0.18039216, 0.20784314, 0.12156863, 0.03529...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76696</th>\n",
       "      <td>4209</td>\n",
       "      <td>149567</td>\n",
       "      <td>0</td>\n",
       "      <td>data/images/1896095186.01.THUMBZZZ.jpg</td>\n",
       "      <td>[[[0.16078432, 0.11372549, 0.21176471, 0.41568...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76697</th>\n",
       "      <td>40779</td>\n",
       "      <td>149568</td>\n",
       "      <td>0</td>\n",
       "      <td>data/images/8408044079.01.THUMBZZZ.jpg</td>\n",
       "      <td>[[[0.1254902, 0.11372549, 0.105882354, 0.09803...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76698</th>\n",
       "      <td>1879</td>\n",
       "      <td>149569</td>\n",
       "      <td>0</td>\n",
       "      <td>data/images/0767907566.01.THUMBZZZ.jpg</td>\n",
       "      <td>[[[0.8980392, 0.9411765, 0.88235295, 0.8549019...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76699 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id    isbn  rating                                img_path  \\\n",
       "0           13       0       0  data/images/0002005018.01.THUMBZZZ.jpg   \n",
       "1        13426       0       0  data/images/0002005018.01.THUMBZZZ.jpg   \n",
       "2        26761       1       0  data/images/0060973129.01.THUMBZZZ.jpg   \n",
       "3        16495       2       0  data/images/0374157065.01.THUMBZZZ.jpg   \n",
       "4         6225       3       0  data/images/0399135782.01.THUMBZZZ.jpg   \n",
       "...        ...     ...     ...                                     ...   \n",
       "76694     7728  149565       0  data/images/1576734218.01.THUMBZZZ.jpg   \n",
       "76695    47785  149566       0  data/images/3492223710.01.THUMBZZZ.jpg   \n",
       "76696     4209  149567       0  data/images/1896095186.01.THUMBZZZ.jpg   \n",
       "76697    40779  149568       0  data/images/8408044079.01.THUMBZZZ.jpg   \n",
       "76698     1879  149569       0  data/images/0767907566.01.THUMBZZZ.jpg   \n",
       "\n",
       "                                              img_vector  \n",
       "0      [[[0.99215686, 0.99607843, 0.9882353, 0.996078...  \n",
       "1      [[[0.99215686, 0.99607843, 0.9882353, 0.996078...  \n",
       "2      [[[0.40784314, 0.59607846, 0.6313726, 0.411764...  \n",
       "3      [[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...  \n",
       "4      [[[0.003921569, 0.003921569, 0.003921569, 0.00...  \n",
       "...                                                  ...  \n",
       "76694  [[[0.003921569, 0.003921569, 0.003921569, 0.00...  \n",
       "76695  [[[0.18039216, 0.20784314, 0.12156863, 0.03529...  \n",
       "76696  [[[0.16078432, 0.11372549, 0.21176471, 0.41568...  \n",
       "76697  [[[0.1254902, 0.11372549, 0.105882354, 0.09803...  \n",
       "76698  [[[0.8980392, 0.9411765, 0.88235295, 0.8549019...  \n",
       "\n",
       "[76699 rows x 5 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2bfe4520-07b4-4852-bbf9-96e2343ad8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_train = df_\n",
    "img_test = df_1\n",
    "\n",
    "data = {\n",
    "        'train':train,\n",
    "        'test':test,\n",
    "        'users':users,\n",
    "        'books':books,\n",
    "        'sub':sub,\n",
    "        'idx2user':idx2user,\n",
    "        'idx2isbn':idx2isbn,\n",
    "        'user2idx':user2idx,\n",
    "        'isbn2idx':isbn2idx,\n",
    "        'img_train':img_train,\n",
    "        'img_test':img_test,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ca8c87d-4602-4783-8dad-e87019d6920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "                                                        data['img_train'][['user_id', 'isbn', 'img_vector']],\n",
    "                                                        data['img_train']['rating'],\n",
    "                                                        test_size=0.2,\n",
    "                                                        random_state=42,\n",
    "                                                        shuffle=True\n",
    "                                                        )\n",
    "data['X_train'], data['X_valid'], data['y_train'], data['y_valid'] = X_train, X_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7d6ae89d-8167-434e-aa95-77a470a1266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image_Dataset(Dataset):\n",
    "    def __init__(self, user_isbn_vector, img_vector, label):\n",
    "        self.user_isbn_vector = user_isbn_vector\n",
    "        self.img_vector = img_vector\n",
    "        self.label = label\n",
    "    def __len__(self):\n",
    "        return self.user_isbn_vector.shape[0]\n",
    "    def __getitem__(self, i):\n",
    "        return {\n",
    "                'user_isbn_vector' : torch.tensor(self.user_isbn_vector[i], dtype=torch.long),\n",
    "                'img_vector' : torch.tensor(self.img_vector[i], dtype=torch.float32),\n",
    "                'label' : torch.tensor(self.label[i], dtype=torch.float32),\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c92e937-e915-49b7-9293-496e2860708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "train_dataset = Image_Dataset(\n",
    "                                data['X_train'][['user_id', 'isbn']].values,\n",
    "                                data['X_train']['img_vector'].values,\n",
    "                                data['y_train'].values\n",
    "                                )\n",
    "valid_dataset = Image_Dataset(\n",
    "                            data['X_valid'][['user_id', 'isbn']].values,\n",
    "                            data['X_valid']['img_vector'].values,\n",
    "                            data['y_valid'].values\n",
    "                            )\n",
    "test_dataset = Image_Dataset(\n",
    "                            data['img_test'][['user_id', 'isbn']].values,\n",
    "                            data['img_test']['img_vector'].values,\n",
    "                            data['img_test']['rating'].values\n",
    "                            )\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=0, shuffle=True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, num_workers=0, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=0, shuffle=False)\n",
    "data['train_dataloader'], data['valid_dataloader'], data['test_dataloader'] = train_dataloader, valid_dataloader, test_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ff3887-76f0-42a2-ad7e-66deb34990ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, field_dims: np.ndarray, embed_dim: int):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(sum(field_dims), embed_dim)\n",
    "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.long)\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight.data)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        :param x: Long tensor of size ``(batch_size, num_fields)``\n",
    "        \"\"\"\n",
    "        x = x + x.new_tensor(self.offsets).unsqueeze(0)\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29481d91-61f4-4166-a45e-7528e8eef1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Base(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(CNN_Base, self).__init__()\n",
    "        self.cnn_layer = nn.Sequential(\n",
    "                                        nn.Conv2d(3, 6, kernel_size=3, stride=2, padding=1),\n",
    "                                        nn.BatchNorm2d(6),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "                                        nn.Conv2d(6, 12, kernel_size=3, stride=2, padding=1),\n",
    "                                        nn.BatchNorm2d(12),\n",
    "                                        nn.ReLU(),\n",
    "                                        )\n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layer(x)\n",
    "        x = x.view(-1, 12 * 4 * 4)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d994bc-4623-49ed-bd32-93e6e12822dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorizationMachine_v(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.v = nn.Parameter(torch.rand(input_dim, latent_dim), requires_grad = True)\n",
    "        self.linear = nn.Linear(input_dim, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        linear = self.linear(x)\n",
    "        square_of_sum = torch.mm(x, self.v) ** 2\n",
    "        sum_of_square = torch.mm(x ** 2, self.v ** 2)\n",
    "        pair_interactions = torch.sum(square_of_sum - sum_of_square, dim=1, keepdim=True)\n",
    "        output = linear + (0.5 * pair_interactions)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1532594f-74af-4ed6-b8fe-2435c725efdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _CNN_FM(torch.nn.Module):\n",
    "    def __init__(self, field_dims, embed_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = FeaturesEmbedding(field_dims, embed_dim)\n",
    "        self.cnn = CNN_Base()\n",
    "        self.fm = FactorizationMachine_v(\n",
    "                                         input_dim=(embed_dim * 2) + (12 * 4 * 4),\n",
    "                                         latent_dim=latent_dim,\n",
    "                                         )\n",
    "\n",
    "    def forward(self, x):\n",
    "        user_isbn_vector, img_vector = x[0], x[1]\n",
    "        user_isbn_feature = self.embedding(user_isbn_vector)\n",
    "        img_feature = self.cnn(img_vector)\n",
    "        feature_vector = torch.cat([\n",
    "                                    user_isbn_feature.view(-1, user_isbn_feature.size(1) * user_isbn_feature.size(2)),\n",
    "                                    img_feature\n",
    "                                    ], dim=1)\n",
    "        output = self.fm(feature_vector)\n",
    "        return output.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57cd3dca-ef89-4108-8ba1-6083dd52fe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_FM:\n",
    "    def __init__(self, args, data):\n",
    "        super().__init__()\n",
    "        self.device = args.DEVICE\n",
    "        self.model = _CNN_FM(\n",
    "                            np.array([len(data['user2idx']), len(data['isbn2idx'])], dtype=np.uint32),\n",
    "                            args.CNN_FM_EMBED_DIM,\n",
    "                            args.CNN_FM_LATENT_DIM\n",
    "                            ).to(self.device)\n",
    "        self.optimizer =  torch.optim.Adam(self.model.parameters(), lr=args.LR)\n",
    "        self.train_data_loader = data['train_dataloader']\n",
    "        self.valid_data_loader = data['valid_dataloader']\n",
    "        self.criterion = RMSELoss()\n",
    "        self.epochs = args.EPOCHS\n",
    "        self.model_name = 'image_model'\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        minimum_loss = 999999999\n",
    "        loss_list = []\n",
    "        tk0 = tqdm.tqdm(range(self.epochs), smoothing=0, mininterval=1.0)\n",
    "        for epoch in tk0:\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            n = 0\n",
    "            for i, data in enumerate(self.train_data_loader):\n",
    "                if len(data) == 2:\n",
    "                    fields, target = [data['user_isbn_vector'].to(self.device)], data['label'].to(self.device)\n",
    "                else:\n",
    "                    fields, target = [data['user_isbn_vector'].to(self.device), data['img_vector'].to(self.device)], data['label'].to(self.device)\n",
    "                y = self.model(fields)\n",
    "                loss = self.criterion(y, target.float())\n",
    "                self.model.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "                n += 1\n",
    "            self.model.eval()\n",
    "            val_total_loss = 0\n",
    "            val_n = 0\n",
    "            for i, data in enumerate(self.valid_data_loader):\n",
    "                if len(data) == 2:\n",
    "                    fields, target = [data['user_isbn_vector'].to(self.device)], data['label'].to(self.device)\n",
    "                else:\n",
    "                    fields, target = [data['user_isbn_vector'].to(self.device), data['img_vector'].to(self.device)], data['label'].to(self.device)\n",
    "                y = self.model(fields)\n",
    "                loss = self.criterion(y, target.float())\n",
    "                self.model.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                val_total_loss += loss.item()\n",
    "                val_n += 1\n",
    "            if minimum_loss > (val_total_loss/val_n):\n",
    "                minimum_loss = (val_total_loss/val_n)\n",
    "                if not os.path.exists('./models'):\n",
    "                    os.makedirs('./models')\n",
    "                torch.save(self.model.state_dict(), './models/{}.pt'.format(self.model_name))\n",
    "                loss_list.append([epoch, total_loss/n, val_total_loss/val_n, 'Model saved'])\n",
    "            else:\n",
    "                loss_list.append([epoch, total_loss/n, val_total_loss/val_n, 'None'])\n",
    "            tk0.set_postfix(train_loss=total_loss/n, valid_loss=val_total_loss/val_n)\n",
    "            wandb.log({\"total_loss\": total_loss/n}, step = epoch)\n",
    "            wandb.log({\"val_loss\": val_total_loss/n}, step = epoch)\n",
    "        return val_total_loss/n\n",
    "\n",
    "    def predict(self, test_data_loader):\n",
    "        self.model.eval()\n",
    "        self.model.load_state_dict(torch.load('./models/{}.pt'.format(self.model_name)))\n",
    "        targets, predicts = list(), list()\n",
    "        with torch.no_grad():\n",
    "            for data in test_data_loader:\n",
    "                if len(data) == 2:\n",
    "                    fields, target = [data['user_isbn_vector'].to(self.device)], data['label'].to(self.device)\n",
    "                else:\n",
    "                    fields, target = [data['user_isbn_vector'].to(self.device), data['img_vector'].to(self.device)], data['label'].to(self.device)\n",
    "                y = self.model(fields)\n",
    "                targets.extend(target.tolist())\n",
    "                predicts.extend(y.tolist())\n",
    "        return predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc81bad-eeaa-4a68-b02f-4a672a9c9aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
